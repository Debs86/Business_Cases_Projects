{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Case #5 - Retail - Pre_processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors:\n",
    "#### DÃ©bora Santos (m20200748),Pedro Henrique Medeiros (m20200742), Rebeca Pinheiro (m20201096)\n",
    "\n",
    "#### Group D - D4B Consulting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing and import packages\n",
    "\n",
    "Maybe it will be necessary install some 'special' packages to this notebook works. \n",
    "\n",
    "Please follow the next cells and check if it's necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT LIBRARIES\n",
    "import sqlite3\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, MeanShift, DBSCAN, estimate_bandwidth\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder, LabelEncoder, RobustScaler\n",
    "from itertools import product\n",
    "from math import ceil\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "%matplotlib inline\n",
    "from pandas_profiling import ProfileReport \n",
    "%config InlineBackend.figure_format = 'retina' \n",
    "from scipy.stats import iqr as IQR\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples, davies_bouldin_score\n",
    "import phik\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "#outliers detection\n",
    "from scipy import stats\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from os.path import join\n",
    "\n",
    "\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Seeting seaborn style\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect initial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-61666ff10bac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#import dataset in csv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'NOVAIMS_MAA_2020e21_BusinessCasesDataScience_MindOverData_RetailChallenge.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#pd.set_option('display.max_rows', 500)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_code\u001b[1;34m(self, code_obj, result, async_)\u001b[0m\n\u001b[0;32m   3425\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowtraceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexception_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3426\u001b[0m             \u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"To exit: use 'exit', 'quit', or Ctrl-D.\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3427\u001b[1;33m         \u001b[1;32mexcept\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcustom_exceptions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3428\u001b[0m             \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3429\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#import dataset in csv\n",
    "df = pd.read_csv('NOVAIMS_MAA_2020e21_BusinessCasesDataScience_MindOverData_RetailChallenge.csv')\n",
    "#pd.set_option('display.max_rows', 500)\n",
    "#pd.set_option('display.max_columns', 500)\n",
    "#pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe, explore and assess data quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First look at the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First look at the dataframe\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types and null values verification\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for all variables\n",
    "df.describe(include='all').transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace possible empty values with NaN\n",
    "df.replace(\"\", np.nan, inplace=True)\n",
    "\n",
    "# Function to show missing values and their percentages\n",
    "def missing_data(df):\n",
    "    l = []\n",
    "    for col in df.columns:\n",
    "        missing = df[col].isnull().sum(axis=0)\n",
    "        percentage = missing/df.shape[0]\n",
    "        l.append([col,missing,percentage*100])\n",
    "        df_missing = pd.DataFrame(l, columns = ['Feature','missing','percentage'])\n",
    "        df_missing = df_missing.sort_values('missing',axis=0, ascending = False)    \n",
    "    return df_missing\n",
    "\n",
    "# Check the percentage of missing values\n",
    "df_missing = missing_data(df)\n",
    "df_missing = df_missing[df_missing['missing']>0]\n",
    "df_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change 'Date' data type to datetime\n",
    "df1['Date'] = df1['Date'].astype('datetime64[ns]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1[df1['Measures']== \"Sell-out units\"]\n",
    "df2.rename(columns = {'Value':'Quantity'}, inplace = True)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df1[df1['Measures']== \"Sell-out values\"]\n",
    "df3.rename(columns = {'Value':'Sales Values'}, inplace = True)\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.drop(['Measures'], axis=1, inplace=True)\n",
    "df3.drop(['Measures'], axis=1, inplace=True)\n",
    "df2.reset_index(inplace=True)\n",
    "df3.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Sales Values'] = df3['Sales Values']\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.drop(columns=['index'])\n",
    "df1 = df2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics \n",
    "df1.describe().apply(lambda s: s.apply(lambda x:format(x,'g')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1[df1['Sales Values'] >0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 ['Avg price'] = df1['Sales Values']/df1['Quantity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(os.path.join(\"df_pre_processed.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metric features\n",
    "metric_features = ['Quantity','Sales Values','Avg price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting file to predict demand forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProductPackSKU_ID</th>\n",
       "      <th>Point-of-Sale_ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Quantity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ProductSKU_1970</td>\n",
       "      <td>POS_1</td>\n",
       "      <td>2017-03-04</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ProductSKU_1970</td>\n",
       "      <td>POS_1</td>\n",
       "      <td>2016-05-02</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ProductSKU_1970</td>\n",
       "      <td>POS_1</td>\n",
       "      <td>2016-10-24</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ProductSKU_1970</td>\n",
       "      <td>POS_1</td>\n",
       "      <td>2017-10-13</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ProductSKU_1970</td>\n",
       "      <td>POS_1</td>\n",
       "      <td>2017-10-14</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91171147</th>\n",
       "      <td>ProductSKU_1813</td>\n",
       "      <td>POS_410</td>\n",
       "      <td>2016-01-28</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91171148</th>\n",
       "      <td>ProductSKU_1813</td>\n",
       "      <td>POS_410</td>\n",
       "      <td>2016-04-20</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91171149</th>\n",
       "      <td>ProductSKU_1813</td>\n",
       "      <td>POS_410</td>\n",
       "      <td>2016-04-25</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91171150</th>\n",
       "      <td>ProductSKU_1813</td>\n",
       "      <td>POS_410</td>\n",
       "      <td>2016-04-28</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91171151</th>\n",
       "      <td>ProductSKU_1813</td>\n",
       "      <td>POS_410</td>\n",
       "      <td>2016-04-29</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91170482 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ProductPackSKU_ID Point-of-Sale_ID       Date  Quantity\n",
       "0          ProductSKU_1970            POS_1 2017-03-04       2.0\n",
       "1          ProductSKU_1970            POS_1 2016-05-02       4.0\n",
       "2          ProductSKU_1970            POS_1 2016-10-24       2.0\n",
       "3          ProductSKU_1970            POS_1 2017-10-13       2.0\n",
       "4          ProductSKU_1970            POS_1 2017-10-14       2.0\n",
       "...                    ...              ...        ...       ...\n",
       "91171147   ProductSKU_1813          POS_410 2016-01-28       1.0\n",
       "91171148   ProductSKU_1813          POS_410 2016-04-20       1.0\n",
       "91171149   ProductSKU_1813          POS_410 2016-04-25       1.0\n",
       "91171150   ProductSKU_1813          POS_410 2016-04-28       1.0\n",
       "91171151   ProductSKU_1813          POS_410 2016-04-29       1.0\n",
       "\n",
       "[91170482 rows x 4 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demand = df1.copy()\n",
    "df_demand = df_demand[['ProductPackSKU_ID','Point-of-Sale_ID','Date','Quantity']]\n",
    "df_demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demand.to_csv(os.path.join(\"df_demand.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAQUI EM DIANTE NÃƒO ESTÃ ATUALIZADO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms for the metric features\n",
    "sns.set()\n",
    "\n",
    "# Prepare figure. Create individual axes where each histogram will be placed\n",
    "fig, axes = plt.subplots(3, ceil(len(metric_features) / 3), figsize=(20, 11))\n",
    "\n",
    "# Plot data\n",
    "# Iterate across axes objects and associate each histogram:\n",
    "for ax, feat in zip(axes.flatten(), metric_features): \n",
    "    ax.hist(df1[feat], color = \"Navy\")\n",
    "    ax.set_title(feat)\n",
    "    \n",
    "# Layout\n",
    "# Add a centered title to the figure:\n",
    "title = \"Numeric Variables Histograms\"\n",
    "plt.suptitle(title)\n",
    "#Adjust the space between plots\n",
    "plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "\n",
    "\n",
    "#Show Plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots for the metric features\n",
    "sns.set()\n",
    "\n",
    "# Prepare figure and create individual axes where each box plot will be placed\n",
    "fig, axes = plt.subplots(3, ceil(len(metric_features) /3), figsize=(20, 11))\n",
    "\n",
    "# Plot the data\n",
    "# Iterate across axes objects and associate each box plot\n",
    "for ax, feat in zip(axes.flatten(), metric_features):\n",
    "    sns.boxplot(df1[feat], ax=ax,color=\"royalblue\")\n",
    "    \n",
    "# Layout\n",
    "# Add a centered title to the figure\n",
    "title = \"Numeric Variables - Box Plots\"\n",
    "\n",
    "plt.suptitle(title)\n",
    "\n",
    "# Adjust the space between plots\n",
    "plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "\n",
    "# Show plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[df1['Avg price']>20000].sort_values(by = 'Avg price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[df1['Avg price']>20000].sort_values(by = 'Avg price')['ProductPackSKU_ID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.groupby(['ProductPackSKU_ID']).mean()['Avg price'].sort_values().tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SKU's 4295,8223, 3242, 4212, 4206 and 3241 don't have average price around 20.000 but they had sales with price above 20.000. Let's check it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[df1['ProductPackSKU_ID']=='ProductSKU_4295'].sort_values(by='Avg price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[df1['ProductPackSKU_ID']=='ProductSKU_8223'].sort_values(by='Avg price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[df1['ProductPackSKU_ID']=='ProductSKU_3242'].sort_values(by='Avg price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[df1['ProductPackSKU_ID']=='ProductSKU_4212'].sort_values(by='Avg price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[df1['ProductPackSKU_ID']=='ProductSKU_4206'].sort_values(by='Avg price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[df1['ProductPackSKU_ID']=='ProductSKU_3241'].sort_values(by='Avg price')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SKUs 4295, 8223, 3241 and 3242 have transactions with prices very different from the others. We are going to remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1[df1['Avg price']<40000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1[~((df1['ProductPackSKU_ID']=='ProductSKU_3241')&(df1['Avg price']>20000))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1[~((df1['ProductPackSKU_ID']=='ProductSKU_3242')&(df1['Avg price']>20000))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1[~((df1['ProductPackSKU_ID']=='ProductSKU_4295')&(df1['Avg price']>30000))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots for the metric features\n",
    "sns.set()\n",
    "\n",
    "# Prepare figure and create individual axes where each box plot will be placed\n",
    "fig, axes = plt.subplots(3, ceil(len(metric_features) /3), figsize=(20, 11))\n",
    "\n",
    "# Plot the data\n",
    "# Iterate across axes objects and associate each box plot\n",
    "for ax, feat in zip(axes.flatten(), metric_features):\n",
    "    sns.boxplot(df1[feat], ax=ax,color=\"royalblue\")\n",
    "    \n",
    "# Layout\n",
    "# Add a centered title to the figure\n",
    "title = \"Numeric Variables - Box Plots\"\n",
    "\n",
    "plt.suptitle(title)\n",
    "\n",
    "# Adjust the space between plots\n",
    "plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "\n",
    "# Remove the last  2 plots\n",
    "#axes.flatten()[-2].remove()\n",
    "#axes.flatten()[-1].remove()\n",
    "# Show plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[df1['Quantity']>150].sort_values(by = 'Quantity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[df1['ProductPackSKU_ID']=='ProductSKU_1572'].sort_values(by='Avg price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardize the data to run the outliers methods\n",
    "#Select data\n",
    "df1_out = df1[metric_features].copy()\n",
    "# Create a StandardScaler\n",
    "scale = StandardScaler()\n",
    "# Fit and Transform data by applying the scale obtained in the previous command\n",
    "scale_feat= scale.fit_transform(df1_out[metric_features])\n",
    "#Applying the transformation in the dataset\n",
    "df1_out[metric_features]=scale_feat\n",
    "#Check the results\n",
    "df1_out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_out = df1[metric_features].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the statistics summary to check it worked\n",
    "df1_out.describe().apply(lambda s: s.apply(lambda x:format(x,'g')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We use 6 methods to recognize outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1) Zscore that checks how many standard deviations is a datapoint distant from the mean;\n",
    "def out_zscore(df, threshold = 3.0):\n",
    "    z = pd.DataFrame(np.abs(stats.zscore(df)), columns = df.columns, index = df.index)\n",
    "    z = pd.DataFrame(z > threshold)\n",
    "    return z.any(axis = 1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2) InterQuantile Range method that creates boundaries using the first and thirs quantile and the interquantile range;\n",
    "def out_iqr(df, multiplier = 1.5):\n",
    "    q75 = df.quantile(0.75, axis = 0)\n",
    "    q25 = df.quantile(0.25, axis = 0)\n",
    "    iqr = q75 - q25\n",
    "    lower_bound = q25 - multiplier*iqr\n",
    "    upper_bound = q75 + multiplier*iqr\n",
    "    return df.apply(lambda x: np.any((x<lower_bound) | (x>upper_bound)), 1).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Local Outlier Factor (LOF) is a function that detects outliers by comparing the density of the neighborhood of a point to the ones of its neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lof = LocalOutlierFactor(contamination = 'auto',metric = 'euclidean',n_neighbors=1)\n",
    "lof_out = pd.Series(lof.fit_predict(df1_out), index = df1_out.index)\n",
    "lof_out.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) At the basis of the Isolation Forest algorithm there is the tendency of anomalous instances in a dataset to be easier to separate from the rest of the sample (isolate), compared to normal points. In order to isolate a data point the algorithm recursively generates partitions on the sample by randomly selecting an attribute and then randomly selecting a split value for the attribute, between the minimum and maximum values allowed for that attribute. When all the trees are grown outliers are identified as points easier to isolate, therefore with a smaller path lenght in the tree, being closer to the root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isolation_forest = IsolationForest(random_state = 7, contamination=  \"auto\")\n",
    "isolation_forest.fit(df1_out) #same results with scaled\n",
    "isofor_outliers = pd.Series(isolation_forest.predict(df1_out), index = df1_out.index)\n",
    "isofor_outliers.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) One-Class SVM is similar to support vector machine, but instead of using a hyperplane to separate two classes of instances, it uses a hypersphere to encompass all of the instances. The algorithm will try to find the smallest possible hypersphere and point outside of it will be considered outliers. One bad thing of this algorithm is that you have to set in advance the percentage of points that you think are outliers, in this case I use the one retrieved from LOF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_fraction =   0.063\n",
    "auto_detection = svm.OneClassSVM(kernel='rbf', gamma=0.01, degree=3, nu=outliers_fraction)\n",
    "auto_detection.fit(df1_out)\n",
    "svm_outliers = pd.Series(auto_detection.predict(df1_out), index = df1_out.index)\n",
    "svm_outliers.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6) DBSCAN set a point p and the density around a point can be measured by the number of points surrounding it. To measure the density around a point p, it used the topological definition of neighborhood. The ðœ€-neighborhood of a point p is the space within a radius ðœ€ > 0 centered in p. For this algorithm, it's necessary set the minimal number of points in each neighborhood (min_samples) and the size of the radius (area) of neighborhood (eps). It's considered a core point when a point there is at least the minimal points around it into your radius. Not core points are points inside the radium of a core point, but they don't have the minimal number of points around them. The noise point are points they are neither a core point and not core point. We are interest in this analyse to detect the noise points. The 2 plots bellow are to find the eps to run the dbscan algorithm.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elbow plot to find the number of neighbors\n",
    "inert = []\n",
    "K = range(1,5)\n",
    "for k in K:\n",
    "    kmeanModel = KMeans(n_clusters=k)\n",
    "    kmeanModel.fit(df1_out)\n",
    "    inert.append(kmeanModel.inertia_)\n",
    "#Define name for x axis and y axis\n",
    "plt.xlabel(\"clusters\")\n",
    "plt.ylabel(\"avg_square distance\")\n",
    "plt.plot(K,inert,\"o-\",color=\"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-distance graph to find out the right eps value\n",
    "#Use the number of neighbors detect in the elbow plot. \n",
    "neigh = NearestNeighbors(n_neighbors=3)\n",
    "neigh.fit(df1_out)\n",
    "distances, _ = neigh.kneighbors(df1_out)\n",
    "distances = np.sort(distances[:, -1])\n",
    "plt.plot(distances)\n",
    "# plt.subplot(1.1,figsize=(20,15))\n",
    "plt.figure(1, figsize=(20, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After find the eps value, we run the DBSCAN algorithm\n",
    "dbscan = DBSCAN(eps=2.5, min_samples=20, n_jobs=-1)\n",
    "dbscan_labels = dbscan.fit_predict(df1_out)\n",
    "dbscan_outliers = pd.Series(dbscan.fit_predict(df1_out), index = df1_out.index)\n",
    "dbscan_outliers.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Including columns with the results of each method in the dataset to compare the different methods and also combine them.\n",
    "df1_out['zscore'] = out_zscore(df1_out)\n",
    "\n",
    "df1_out['iqr'] = out_iqr(df1_out, multiplier = 3.0)\n",
    "\n",
    "df1_out['lof_out'] = [0 for i in range(len(df1_out))]\n",
    "df1_out.loc[lof_out[lof_out == -1].index, 'lof_out'] = 1\n",
    "\n",
    "df1_out['isofor_out'] = [0 for i in range(len(df1_out))]\n",
    "df1_out.loc[isofor_outliers[isofor_outliers == -1].index, 'isofor_out'] = 1\n",
    "\n",
    "df1_out['svm_out'] = [0 for i in range(len(df1_out))]\n",
    "df1_out.loc[svm_outliers[svm_outliers == -1].index, 'svm_out'] = 1\n",
    "\n",
    "df1_out['dbscan_out'] = [0 for i in range(len(df1_out))]\n",
    "df1_out.loc[dbscan_outliers[dbscan_outliers != -1].index, 'dbscan_out'] = 0\n",
    "df1_out.loc[dbscan_outliers[dbscan_outliers == -1].index, 'dbscan_out'] = 1\n",
    "\n",
    "df1_out['sum_out']  = df1_out[['zscore','iqr','svm_out','lof_out','isofor_out','dbscan_out']].apply(lambda x: sum(x), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dataset for see the outliers detect by each model\n",
    "outliers = df1_out[['zscore','iqr','svm_out','lof_out','isofor_out','dbscan_out']]\n",
    "outliers.groupby(['zscore','iqr','svm_out','lof_out','isofor_out','dbscan_out'])\\\n",
    "            .size()\\\n",
    "            .to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_out['iqr'] = out_iqr(df1_out, multiplier = 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show the number of outliers detect by z-score individually\n",
    "df1_out['zscore'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show the number of outliers detect by IQR individually\n",
    "df1_out['iqr'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show the number of outliers detect by combined methods. \n",
    "df1_out['sum_out'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chose the number 4 with the best possible solution of combined methods\n",
    "df1_out2 = df1.loc[df1_out.query('sum_out == 4').index]\n",
    "df2 = df1.drop(index = df1_out.query('sum_out == 4').index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at the results\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check boxplots after outliers removal\n",
    "# Box plots for the metric features\n",
    "sns.set()\n",
    "\n",
    "# Prepare figure and create individual axes where each box plot will be placed\n",
    "fig, axes = plt.subplots(4, ceil(len(metric_features) /4), figsize=(20, 11))\n",
    "\n",
    "# Plot the data\n",
    "# Iterate across axes objects and associate each box plot\n",
    "for ax, feat in zip(axes.flatten(), metric_features):\n",
    "    sns.boxplot(df2[feat], ax=ax,color=\"royalblue\")\n",
    "    \n",
    "# Layout\n",
    "# Add a centered title to the figure\n",
    "title = \"Numeric Variables - Box Plots\"\n",
    "\n",
    "plt.suptitle(title)\n",
    "\n",
    "# Adjust the space between plots\n",
    "plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "\n",
    "# Remove the last  2 plots\n",
    "axes.flatten()[-2].remove()\n",
    "axes.flatten()[-1].remove()\n",
    "# Show plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After testing removing based on 4 combined methods and realising that did not seem to make a big difference on the box plots in terms of outliers, we have dropped this approach and decided to apply Inter quartile range (IQR) only on five features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the features Drywh, Freq the best multiplier is 1 and for the features Dessert,Sweetred,Sweetwh, the best multiplier is 4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create a copy of the dataset\n",
    "df1_iqr = df1.copy()\n",
    "#Define the features to apply the IQR method\n",
    "iqr_features= ['Drywh','Freq']\n",
    "#Run the function setting the multiplier =1 and create a new columns to store the results\n",
    "df1_iqr['out'] = out_iqr(df1_iqr[iqr_features], multiplier = 1)\n",
    "#Show the results\n",
    "df1_iqr['out'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter the data excluding the outliers detected\n",
    "df1_iqr1 = df1_iqr[df1_iqr['out']==0]\n",
    "#Create a dataset to store the outliers detected\n",
    "df1_iqr_out = df1_iqr[df1_iqr['out']==1]\n",
    "#Drop the column create in both datasets\n",
    "df1_iqr1.drop(['out'], axis=1, inplace=True)\n",
    "df1_iqr_out.drop(['out'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Second step\n",
    "#Define the features to apply the IQR method\n",
    "iqr_features2= ['Dessert','Sweetred','Sweetwh']\n",
    "#Run the function setting the multiplier =4.5 and create a new columns to store the results\n",
    "df1_iqr1['out'] = out_iqr(df1_iqr1[iqr_features2], multiplier = 4.5)\n",
    "#Show the results\n",
    "df1_iqr1['out'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter the data excluding the outliers detected\n",
    "df1_iqr2 = df1_iqr1[df1_iqr1['out']==0]\n",
    "#Create a dataset to the outliers detected\n",
    "df1_iqr_out2 = df1_iqr1[df1_iqr1['out']==1]\n",
    "#Drop the column create in both datasets\n",
    "df1_iqr2.drop(['out'], axis=1, inplace=True)\n",
    "df1_iqr_out2.drop(['out'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Percentage of data kept after removing outliers:', np.round(df1_iqr2.shape[0] / df1.shape[0], 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the results\n",
    "df1_iqr2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging the both datasets used to store the outliers detected\n",
    "df1_out_final = df1_iqr_out.append(df1_iqr_out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the outliers dataset\n",
    "df1_out_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check boxplots after outliers removal\n",
    "# Box plots for the metric features\n",
    "sns.set()\n",
    "\n",
    "# Prepare figure and create individual axes where each box plot will be placed\n",
    "fig, axes = plt.subplots(4, ceil(len(metric_features) /4), figsize=(20, 11))\n",
    "\n",
    "# Plot the data\n",
    "# Iterate across axes objects and associate each box plot\n",
    "for ax, feat in zip(axes.flatten(), metric_features):\n",
    "    sns.boxplot(df1_iqr2[feat], ax=ax,color=\"royalblue\")\n",
    "    \n",
    "# Layout\n",
    "# Add a centered title to the figure\n",
    "title = \"Numeric Variables - Box Plots\"\n",
    "\n",
    "plt.suptitle(title)\n",
    "\n",
    "# Adjust the space between plots\n",
    "plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "\n",
    "# Remove the last  2 plots\n",
    "axes.flatten()[-2].remove()\n",
    "axes.flatten()[-1].remove()\n",
    "# Show plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace df1 by the method we decided\n",
    "#IQR for the  5 variables. \n",
    "df1 = df1_iqr2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the data\n",
    "df1.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
